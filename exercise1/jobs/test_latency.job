#!/bin/bash

#SBATCH --job-name=latency

# Define the number of nodes you need.
#SBATCH --nodes=2

# # Define the number of tasks you need. Use with distributed parallelism
# #SBATCH --ntasks=4

# # Eventually, you can further specify the number of tasks per node
# #SBATCH --ntasks-per-node=16

# # Define the number of CPUs allocated to each task. Use with shared memory parallelism
# #SBATCH --cpus-per-task=2

# Define how long the job will run in real time. Format is d-hh:mm:ss
# For a 30 seconds job
#SBATCH --time=0-01:35:00

# Define the account name, e.g. for the Laboratory of Data Engineering
#SBATCH -A dssc

# Define the partition on which the job shall run, e.g. EPYC, THIN, GPU, DGX
#SBATCH -p EPYC

# Define how much memory you need. Choose one between the following
# --mem will define memory per node
# --mem-per-cpu will define memory per CPU/core
# #SBATCH --mem-per-cpu=1500MB
# #SBATCH --mem=5GB    # this one is not in effect, due to the double hash

# Specify the output and error files
# #SBATCH --output=%x.%j.out
# #SBATCH --error=%x.%j.err

# Eventually, you can turn on mail notification.
# Among the possibilities we can list: NONE, BEGIN, END, FAIL, ALL
# #SBATCH --mail-type=BEGIN,END
# #SBATCH --mail-user=fifo@lifo.com

# Pick nodes with feature 'foo'. Different clusters have different features available.
# Most of the time you don't need this
# #SBATCH -C foo

# Restrict the job to run on the node(s) named
# #SBATCH -w epyc008

# Start the program

>&2 echo "DIR is ${SLURM_SUBMIT_DIR}"

dt=$(date +"%Y%m%d_%H%M%S")
for i in {128..256..2}
do
mpirun --cpu-list 0,$i -np 2 osu-micro-benchmarks-7.3/c/mpi/pt2pt/standard/osu_latency -x 100 -i 1000 -m 2:2 >> ${dt}_latency_epyc_msize2.txt
# mpirun --display-map --do-not-launch --cpu-set 0,$i -np 2 osu-micro-benchmarks-7.3/c/mpi/pt2pt/standard/osu_latency -x 100 -i 1000 -m 2:2 >> ${dt}_latency_epyc_msize2.txt
done
# mpirun --cpu-list 0,24 -np 2 osu-micro-benchmarks-7.3/c/mpi/pt2pt/standard/osu_latency -x 100 -i 1000 -m 2:2 | tail -1 | awk '{print $2}' >> ${dt}_latency_thin_msize2.txt